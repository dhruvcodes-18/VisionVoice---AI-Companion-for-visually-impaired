# VisionVoice---AI-Companion-for-visually-impaired
An AI-powered assistive system with voice control, real-time object detection using state of the art gemini AI model, and speech emotion analysis.

# VisionVoice: AI Companion for the Visually Impaired

An AI-powered assistive system designed to enhance digital and physical accessibility for visually impaired users. This project integrates voice-controlled web navigation, real-time object detection, and speech emotion recognition into one unified system that empowers users to interact naturally with both the web and their physical environment.

---

## ğŸ§  Overview

**VisionVoice** is composed of **three independent modules**, each targeting a specific accessibility challenge:

1. ğŸ™ï¸ **Module 1 â€“ VoiceMate**  
   A fully voice-controlled ReactJS website interface that allows users to navigate the web using spoken commands, enhancing screen-free access to digital content.

2. ğŸ‘ï¸ **Module 2 â€“ Gemini Vision (Real-Time Object Detection)**  
   An AI module that uses Googleâ€™s Gemini model to detect objects in the user's surroundings via a webcam, then converts the results into audio feedback, helping users understand their environment.

3. ğŸ—£ï¸ **Module 3 â€“ Speech Emotion Recognition**  
   A machine learning-based system that detects the emotional tone of a speaker from audio input, useful in gauging user well-being or adapting system responses based on emotion.

---

## ğŸ§© Project Architecture

[User Voice Input]
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VoiceMate â”‚ â† ReactJS website with voice command control
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gemini Vision â”‚ â† Object detection + audio captioning via webcam
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Speech Emotion Analyzerâ”‚ â† Sentiment classification from voice clips
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
[User Feedback (Audio/Visual)]


---

## ğŸ”§ Technologies Used

| Module                      | Technologies / Frameworks                                      |
|----------------------------|------------------------------------------------------------------|
| VoiceMate (Module 1)       | ReactJS, JavaScript, SCSS, SpeechRecognition API                |
| Gemini Vision (Module 2)   | Python, OpenCV, Gemini Vision API, pyttsx3, SpeechRecognition   |
| Emotion Recognition (Mod 3)| Python, Keras, TensorFlow, Streamlit, librosa, sklearn          |
| General                    | Git, VS Code, GitHub, Google Colab, Ngrok (for local server tunneling) |

---

## ğŸš€ How to Run Each Module

### 1ï¸âƒ£ Module 1: VoiceMate (Voice Controlled Website)
```bash
cd mod-3-Voicemate
npm install
npm run dev

Open browser to http://localhost:5173

Speak supported commands like â€œGo to Homeâ€, â€œOpen Aboutâ€, â€œSearch for booksâ€, etc.

2ï¸âƒ£ Module 2: Gemini Vision (Object Detection)

cd mod-1-gemini-vision
pip install -r requirements.txt
python script.py


3ï¸âƒ£ Module 3: Speech Emotion Recognition

cd mod-2-speech-emotion-recognition
pip install -r packages.txt
streamlit run app.py

ğŸ“ Folder Structure

VisionVoice---AI-Companion-for-visually-impaired/
â”œâ”€â”€ mod-1-gemini-vision/
â”‚   â”œâ”€â”€ script.py
â”‚   â”œâ”€â”€ libs/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ mod-2-speech-emotion-recognition/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ model.ipynb
â”‚   â”œâ”€â”€ speech_emo_recognition.h5
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ mod-3-Voicemate/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md  â† (You are here)



ğŸŒŸ Features
Modular Architecture â€“ Each component can run independently or be integrated into a larger assistive framework.

Multimodal Accessibility â€“ Combines voice, vision, and emotion to support more natural user interactions.

Fully Open Source â€“ Designed for academic and non-commercial use. Contributions welcome!

ğŸ“ˆ Future Scope
Integrate all modules into one unified application using a Flask or Node.js backend.

Add dynamic voice feedback to emotion recognition.

Expand voice commands using NLP for more natural interactions.

Integrate obstacle detection using LiDAR or ultrasonic sensors.

ğŸ™‹â€â™‚ï¸ Author
Dhruv Sinha
B.Tech in Information Technology â€“ VIT Vellore
Email: dhruvsinha18@gmail.com
GitHub: @dhruvcodes-18



